/**
 * OpenAI model provider implementation.
 *
 * This module provides integration with OpenAI's Chat Completions API,
 * supporting streaming responses, tool use, and configurable model parameters.
 *
 * @see https://platform.openai.com/docs/api-reference/chat/create
 */
import OpenAI, { type ClientOptions } from 'openai';
import type { ApiKeySetter } from 'openai/client';
import { Model } from '../models/model.js';
import type { BaseModelConfig, StreamOptions } from '../models/model.js';
import type { Message } from '../types/messages.js';
import type { ModelStreamEvent } from '../models/streaming.js';
/**
 * Configuration interface for OpenAI model provider.
 *
 * Extends BaseModelConfig with OpenAI-specific configuration options
 * for model parameters and request settings.
 *
 * @example
 * ```typescript
 * const config: OpenAIModelConfig = {
 *   modelId: 'gpt-4o',
 *   temperature: 0.7,
 *   maxTokens: 1024
 * }
 * ```
 */
export interface OpenAIModelConfig extends BaseModelConfig {
    /**
     * OpenAI model identifier (e.g., gpt-4o, gpt-3.5-turbo).
     */
    modelId?: string;
    /**
     * Controls randomness in generation.
     *
     * @see https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature
     */
    temperature?: number;
    /**
     * Maximum number of tokens to generate in the completion.
     *
     * @see https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_completion_tokens
     */
    maxTokens?: number;
    /**
     * Controls diversity via nucleus sampling.
     *
     * @see https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p
     */
    topP?: number;
    /**
     * Reduces repetition of token sequences (-2.0 to 2.0).
     */
    frequencyPenalty?: number;
    /**
     * Encourages the model to talk about new topics (-2.0 to 2.0).
     */
    presencePenalty?: number;
    /**
     * Additional parameters to pass through to the OpenAI API.
     * This field provides forward compatibility for any new parameters
     * that OpenAI introduces. All properties in this object will be
     * spread into the API request.
     *
     * @example
     * ```typescript
     * // Pass stop sequences
     * { params: { stop: ['END', 'STOP'] } }
     *
     * // Pass any future OpenAI parameters
     * { params: { newParameter: 'value' } }
     * ```
     */
    params?: Record<string, unknown>;
}
/**
 * Options interface for creating an OpenAIModel instance.
 */
export interface OpenAIModelOptions extends OpenAIModelConfig {
    /**
     * OpenAI API key (falls back to OPENAI_API_KEY environment variable).
     *
     * Accepts either a static string or an async function that resolves to a string.
     * When a function is provided, it is invoked before each request, allowing for
     * dynamic API key rotation or runtime credential refresh.
     */
    apiKey?: string | ApiKeySetter;
    /**
     * Pre-configured OpenAI client instance.
     * If provided, this client will be used instead of creating a new one.
     */
    client?: OpenAI;
    /**
     * Additional OpenAI client configuration.
     * Only used if client is not provided.
     */
    clientConfig?: ClientOptions;
}
/**
 * OpenAI model provider implementation.
 *
 * Implements the Model interface for OpenAI using the Chat Completions API.
 * Supports streaming responses, tool use, and comprehensive configuration.
 *
 * @example
 * ```typescript
 * const provider = new OpenAIModel({
 *   apiKey: 'sk-...',
 *   modelId: 'gpt-4o',
 *   temperature: 0.7,
 *   maxTokens: 1024
 * })
 *
 * const messages: Message[] = [
 *   { role: 'user', content: [{ type: 'textBlock', text: 'Hello!' }] }
 * ]
 *
 * for await (const event of provider.stream(messages)) {
 *   if (event.type === 'modelContentBlockDeltaEvent' && event.delta.type === 'textDelta') {
 *     process.stdout.write(event.delta.text)
 *   }
 * }
 * ```
 */
export declare class OpenAIModel extends Model<OpenAIModelConfig> {
    private _config;
    private _client;
    /**
     * Creates a new OpenAIModel instance.
     *
     * @param options - Configuration for model and client (modelId is required)
     *
     * @example
     * ```typescript
     * // Minimal configuration with API key and model ID
     * const provider = new OpenAIModel({
     *   modelId: 'gpt-4o',
     *   apiKey: 'sk-...'
     * })
     *
     * // With additional model configuration
     * const provider = new OpenAIModel({
     *   modelId: 'gpt-4o',
     *   apiKey: 'sk-...',
     *   temperature: 0.8,
     *   maxTokens: 2048
     * })
     *
     * // Using environment variable for API key
     * const provider = new OpenAIModel({
     *   modelId: 'gpt-3.5-turbo'
     * })
     *
     * // Using function-based API key for dynamic key retrieval
     * const provider = new OpenAIModel({
     *   modelId: 'gpt-4o',
     *   apiKey: async () => await getRotatingApiKey()
     * })
     *
     * // Using a pre-configured client instance
     * const client = new OpenAI({ apiKey: 'sk-...', timeout: 60000 })
     * const provider = new OpenAIModel({
     *   modelId: 'gpt-4o',
     *   client
     * })
     * ```
     */
    constructor(options?: OpenAIModelOptions);
    /**
     * Updates the model configuration.
     * Merges the provided configuration with existing settings.
     *
     * @param modelConfig - Configuration object with model-specific settings to update
     *
     * @example
     * ```typescript
     * // Update temperature and maxTokens
     * provider.updateConfig({
     *   temperature: 0.9,
     *   maxTokens: 2048
     * })
     * ```
     */
    updateConfig(modelConfig: OpenAIModelConfig): void;
    /**
     * Retrieves the current model configuration.
     *
     * @returns The current configuration object
     *
     * @example
     * ```typescript
     * const config = provider.getConfig()
     * console.log(config.modelId)
     * ```
     */
    getConfig(): OpenAIModelConfig;
    /**
     * Streams a conversation with the OpenAI model.
     * Returns an async iterable that yields streaming events as they occur.
     *
     * @param messages - Array of conversation messages
     * @param options - Optional streaming configuration
     * @returns Async iterable of streaming events
     *
     * @throws \{ContextWindowOverflowError\} When input exceeds the model's context window
     *
     * @example
     * ```typescript
     * const provider = new OpenAIModel({ modelId: 'gpt-4o', apiKey: 'sk-...' })
     * const messages: Message[] = [
     *   { role: 'user', content: [{ type: 'textBlock', text: 'What is 2+2?' }] }
     * ]
     *
     * for await (const event of provider.stream(messages)) {
     *   if (event.type === 'modelContentBlockDeltaEvent' && event.delta.type === 'textDelta') {
     *     process.stdout.write(event.delta.text)
     *   }
     * }
     * ```
     *
     * @example
     * ```typescript
     * // With tool use
     * const options: StreamOptions = {
     *   systemPrompt: 'You are a helpful assistant',
     *   toolSpecs: [calculatorTool]
     * }
     *
     * for await (const event of provider.stream(messages, options)) {
     *   if (event.type === 'modelMessageStopEvent' && event.stopReason === 'toolUse') {
     *     console.log('Model wants to use a tool')
     *   }
     * }
     * ```
     */
    stream(messages: Message[], options?: StreamOptions): AsyncIterable<ModelStreamEvent>;
    /**
     * Formats a request for the OpenAI Chat Completions API.
     *
     * @param messages - Conversation messages
     * @param options - Stream options
     * @returns Formatted OpenAI request
     */
    private _formatRequest;
    /**
     * Formats messages for OpenAI API.
     * Handles splitting tool results into separate messages.
     *
     * @param messages - SDK messages
     * @returns OpenAI-formatted messages
     */
    private _formatMessages;
    /**
     * Converts a snake_case string to camelCase.
     * Used for mapping OpenAI stop reasons to SDK format.
     *
     * @param str - Snake case string (e.g., 'content_filter')
     * @returns Camel case string (e.g., 'contentFilter')
     *
     * @example
     * ```typescript
     * _snakeToCamel('context_length_exceeded') // => 'contextLengthExceeded'
     * _snakeToCamel('tool_calls') // => 'toolCalls'
     * ```
     */
    private _snakeToCamel;
    /**
     * Maps an OpenAI chunk to SDK streaming events.
     *
     * @param chunk - OpenAI chunk
     * @param streamState - Mutable state object tracking message and content block state
     * @param activeToolCalls - Map tracking active tool calls by index
     * @returns Array of SDK streaming events
     */
    private _mapOpenAIChunkToSDKEvents;
}
//# sourceMappingURL=openai.d.ts.map